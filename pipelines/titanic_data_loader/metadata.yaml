blocks:
- all_upstream_blocks_executed: true
  color: null
  configuration: {}
  downstream_blocks:
  - clean_headers
  executor_config: null
  executor_type: local_python
  has_callback: false
  language: python
  name: Load Titanic Data
  retry_config: null
  status: not_executed
  timeout: null
  type: data_loader
  upstream_blocks: []
  uuid: load_titanic_data
- all_upstream_blocks_executed: false
  color: null
  configuration: {}
  downstream_blocks:
  - export_to_bigquery
  executor_config: null
  executor_type: local_python
  has_callback: false
  language: python
  name: Clean Headers
  retry_config: null
  status: not_executed
  timeout: null
  type: transformer
  upstream_blocks:
  - load_titanic_data
  uuid: clean_headers
- all_upstream_blocks_executed: false
  color: null
  configuration: {}
  downstream_blocks: []
  executor_config: null
  executor_type: local_python
  has_callback: false
  language: python
  name: Export to BigQuery
  retry_config: null
  status: not_executed
  timeout: null
  type: data_exporter
  upstream_blocks:
  - clean_headers
  uuid: export_to_bigquery
cache_block_output_in_memory: false
callbacks: []
concurrency_config: {}
conditionals: []
created_at: '2025-06-12 21:12:45.704687+00:00'
created_by: freem124
data_integration: null
description: This data pipeline is designed to extract data from the titanic dataset,
  perform header cleaning through a transformation step, and then export the processed
  data to bigquery. the pipeline begins with data ingestion from the source, which
  may be a csv file or database containing the titanic dataset. next, a transformation
  function standardizes column headers—such as converting to lowercase, removing spaces,
  or renaming columns for clarity—to ensure consistency and compatibility with downstream
  processes. after cleansing the headers, the transformed data is loaded into bigquery,
  facilitating scalable storage and analytical querying. overall, the pipeline automates
  data extraction, preprocessing, and loading, supporting data analysis and modeling
  tasks related to titanic passenger information.
executor_config: {}
executor_count: 1
executor_type: null
extensions: {}
name: Titanic Data Loader
notification_config: {}
overrides: null
remote_variables_dir: null
retry_config: {}
run_pipeline_in_one_process: false
settings:
  triggers: null
spark_config: {}
state_store_config: {}
tags: []
type: python
uuid: titanic_data_loader
variables_dir: /home/src/mage_data/cole-ws
widgets: []
